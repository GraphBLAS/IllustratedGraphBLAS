{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Illustrated GraphBLAS","text":""},{"location":"Chapter0/","title":"Introduction","text":"<p>Interactive Notebook</p>"},{"location":"Chapter0/#summary","title":"Summary","text":"<p>This chapter introduces GraphBLAS and the foundations of algebraic graph processing:</p> <ul> <li>What is GraphBLAS - A sparse linear algebra API for expressing graph algorithms as matrix operations, enabling high-performance computation on CPUs and GPUs</li> <li>Sparse Matrices - Understanding sparse data structures that store only non-zero elements, avoiding wasteful computation with zeros</li> <li>Adjacency Matrices - How graphs are represented as matrices, where non-zero entries correspond to weighted edges between nodes</li> <li>Matrix-Vector Multiplication - The core operation for graph traversal, showing how multiplying a vector by an adjacency matrix discovers neighboring nodes</li> <li>Dense vs Sparse Efficiency - Why sparse algorithms dramatically outperform dense operations on real-world graph data</li> <li>Applications - Overview of graph applications across scientific domains including structural analysis, optimization, quantum chemistry, and economic planning</li> </ul>"},{"location":"Chapter1/","title":"Python GraphBLAS","text":"<p>Interactive Notebook</p>"},{"location":"Chapter1/#summary","title":"Summary","text":"<p>This chapter covers installing and using GraphBLAS libraries with practical examples:</p> <ul> <li>Installation - Installing Python-GraphBLAS via pip, with SuiteSparse as the underlying implementation</li> <li>Language Bindings - Overview of GraphBLAS bindings for Python, Julia, and PostgreSQL</li> <li>COO Format - Creating sparse vectors and matrices using coordinate (COO) format with indices and values arrays</li> <li>Sparse Representation - Understanding why zeros are not stored in sparse data structures</li> <li>Matrix-Vector Multiplication - Using the <code>mxv</code> operation and <code>@</code> operator shorthand for graph traversal</li> <li>GraphBLAS Assignment - The <code>&lt;&lt;</code> operator for writing results back to vectors and matrices</li> <li>Breadth-First Search - Implementing BFS as repeated matrix-vector multiplication, discovering frontier neighbors through linear algebra</li> </ul>"},{"location":"Chapter2/","title":"Semirings and Accumulation","text":"<p>Interactive Notebook</p>"},{"location":"Chapter2/#summary","title":"Summary","text":"<p>This chapter explores how semirings customize matrix multiplication for different problems:</p> <ul> <li>PLUS_TIMES Semiring - Standard arithmetic for computing total costs, such as manufacturing production costs</li> <li>MIN_PLUS Semiring - Finding shortest paths by adding distances and keeping the minimum, used in routing and navigation</li> <li>ANY_PAIR Semiring - Testing reachability and connectivity without computing actual values, enabling early termination</li> <li>Semiring Comparison - How the same matrix structure produces different results depending on the chosen semiring</li> <li>Common Semirings - Reference table of PLUS_TIMES, MIN_PLUS, MAX_PLUS, ANY_PAIR, and PLUS_PAIR with their use cases</li> <li>Accumulation - How new results combine with existing data using accumulators (replacement, PLUS, MIN)</li> <li>Single Source Shortest Path - Complete algorithm demonstration using MIN_PLUS semiring with MIN accumulator, showing step-by-step convergence</li> </ul>"},{"location":"Chapter3/","title":"Masking and BFS","text":"<p>Interactive Notebook</p>"},{"location":"Chapter3/#summary","title":"Summary","text":"<p>This chapter builds a complete Breadth-First Search implementation using GraphBLAS primitives:</p> <ul> <li>Vector-Matrix Multiply - Deep dive into <code>vxm</code> as the core operation for discovering graph neighbors using ANY_PAIR semiring</li> <li>Structural Masks - Selecting output positions based on where a mask has values</li> <li>Value Masks - Selecting output positions where mask values are True</li> <li>Complement Masks - Using <code>~</code> to invert selection, keeping only positions where the mask has no values</li> <li>Masked Expansion - Combining <code>vxm</code> with complement masking to prevent revisiting nodes in a single operation</li> <li>Replacement Semantics - The difference between merge mode (preserving existing values) and replace mode (clearing before writing)</li> <li>BFS Algorithm Construction - Step-by-step refinement from basic expansion to complete BFS with masking and replacement</li> <li>Algorithm Trace - Full walkthrough on a six-node graph showing vector and graph state at each iteration</li> <li>Generalization - How the same pattern applies to SSSP, connected components, PageRank, and triangle counting</li> </ul>"},{"location":"Chapter4/","title":"Matrix-Matrix Multiplication","text":"<p>Interactive Notebook</p>"},{"location":"Chapter4/#summary","title":"Summary","text":"<p>This chapter extends from vector operations to matrix-matrix multiplication for multi-hop path discovery:</p> <ul> <li>Recall: Vector-Matrix Multiply - Review of finding 1-hop neighbors using frontier vectors and adjacency matrices</li> <li>Matrix-Matrix Multiply - Computing A x A to discover 2-hop paths, with GraphBLAS syntax <code>mxm</code> and <code>@</code> operator</li> <li>Multiplication Mechanics - Step-by-step computation showing row-by-row multiplication and sparse optimization</li> <li>Graph Interpretation - How matrix entries translate to paths: direct edges vs multi-hop connections</li> <li>Higher Powers - Computing A\u00b2, A\u00b3, and A^k for k-hop path counts</li> <li>Diagonal Entries - Understanding self-loops in powers as cycle counts (degree in A\u00b2)</li> <li>Transitive Closure - Finding all reachable pairs through iterative multiplication with ANY_PAIR semiring until fixed point</li> <li>Applications - Database query planning, network analysis, compiler optimization, and dependency resolution</li> <li>Preview: Triangle Counting - Introduction to using Hadamard product (A\u00b2 \u2299 A) for detecting triangles</li> </ul>"},{"location":"Chapter5/","title":"Incidence Matrices","text":"<p>Interactive Notebook</p>"},{"location":"Chapter5/#summary","title":"Summary","text":"<p>This chapter introduces incidence matrices as an alternative graph representation:</p> <ul> <li>Incidence vs Adjacency - Adjacency matrices have nodes as both rows and columns; incidence matrices have nodes as rows and edges as columns</li> <li>Incidence Matrix Structure - Each column represents an edge, with non-zero entries indicating which nodes participate in that edge</li> <li>Matrix Multiplication with Incidence - Multiplying incidence matrices to derive relationships and construct new graph structures</li> <li>Multi-graphs - Graphs with multiple edges between the same pair of nodes, naturally represented through incidence matrices</li> <li>Hypergraphs - Generalized graphs where edges can connect more than two nodes, enabling complex relationship modeling</li> <li>Applications - Network flow, bipartite matching, and modeling relationships that go beyond simple pairwise connections</li> </ul>"},{"location":"Chapter6/","title":"Element-wise Operations and Transformations","text":"<p>Interactive Notebook</p>"},{"location":"Chapter6/#summary","title":"Summary","text":"<p>This chapter covers element-wise operations and matrix transformations in GraphBLAS:</p> <ul> <li>Element-wise Addition - Combining matrices by adding values at matching positions, useful for computing graph unions</li> <li>Element-wise Multiplication - The Hadamard product, multiplying matrices element by element to find graph intersections</li> <li>Graph Union and Intersection - Using element-wise operations to combine or filter graph structures</li> <li>Selection Operations - Choosing elements based on criteria such as thresholds, comparisons, or patterns</li> <li>Apply Operations - Transforming matrix elements by applying a function uniformly across the entire matrix</li> <li>Filtering Graphs - Removing edges that don't meet certain criteria</li> <li>Value Transformation - Scaling, normalizing, or otherwise modifying edge weights</li> <li>Practical Applications - Graph comparison, filtering by edge weight, normalizing adjacency matrices</li> </ul>"},{"location":"Chapter7/","title":"Shortest Path","text":"<p>Interactive Notebook</p>"},{"location":"Chapter7/#summary","title":"Summary","text":"<p>This chapter presents an elegant algebraic solution to shortest path problems:</p> <ul> <li>Tropical Semiring - The MIN_PLUS semiring, widely used in optimization problems across engineering and science</li> <li>Single Source Shortest Path - Finding minimum distances from one node to all others</li> <li>All-Pairs Shortest Path - Extending to find shortest paths between all node pairs</li> <li>Algorithm Convergence - Understanding when the iterative process reaches the optimal solution</li> <li>Practical Applications - Navigation, network routing, logistics, and resource optimization</li> </ul>"},{"location":"Chapter8/","title":"Triangle Counting and Centrality","text":"<p>Interactive Notebook</p>"},{"location":"Chapter8/#summary","title":"Summary","text":"<p>This chapter explores triangle structures and node importance in graphs:</p> <ul> <li>Triangles in Graphs - Three mutually connected nodes forming the fundamental building block for graph analysis</li> <li>Algebraic Triangle Counting - Using matrix operations (A\u00b2 \u2299 A) to efficiently count triangles</li> <li>Triangle Counting Algorithms - Different approaches with various performance characteristics</li> <li>Centrality Measures - Ranking node importance based on structural position in the graph</li> <li>Triangle-based Centrality - Determining node importance by counting triangles around each node</li> <li>Applications - Social network analysis, community detection, and graph clustering</li> </ul>"},{"location":"Chapter9/","title":"Sparse Neural Networks","text":"<p>Interactive Notebook</p>"},{"location":"Chapter9/#summary","title":"Summary","text":"<p>This chapter demonstrates how GraphBLAS enables sparse neural network computation:</p> <ul> <li>Neural Network Fundamentals - The building blocks of machine learning and AI systems</li> <li>Dense vs Sparse Networks - How sparse representations overcome memory limitations of dense neural networks</li> <li>Sparse Matrix Representation - Encoding neural network weights as sparse matrices</li> <li>Inference with GraphBLAS - Performing forward propagation using sparse matrix-vector multiplication</li> <li>Activation Functions - Applying non-linear transformations using GraphBLAS apply operations</li> <li>Scalability - Building larger networks than possible with dense approaches</li> </ul>"}]}